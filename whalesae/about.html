<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>About - WhaleSAE</title>
  <link rel="stylesheet" href="style.css">
  <style>
    .method-section {
      background: #fafafa;
      border: 1px solid #eee;
      border-radius: 8px;
      padding: 1.2em 1.4em;
      margin-bottom: 1em;
    }
    .method-section h2 {
      margin-top: 0;
      margin-bottom: 0.3em;
    }
    .method-section p {
      color: #444;
      line-height: 1.65;
    }
    .method-section p:last-child { margin-bottom: 0; }
    .ref-list {
      list-style: none;
      padding: 0;
      margin: 0;
    }
    .ref-list li {
      font-size: 0.88em;
      color: #444;
      line-height: 1.6;
      padding: 0.4em 0;
      border-bottom: 1px solid #f0f0f0;
    }
    .ref-list li:last-child { border-bottom: none; }
    .ref-list .ref-title { font-style: italic; }
    .diagram {
      font-family: "SF Mono", "Fira Code", monospace;
      font-size: 0.78em;
      line-height: 1.6;
      background: #f8f9fa;
      padding: 0.8em 1em;
      border-radius: 4px;
      overflow-x: auto;
      color: #444;
    }
  </style>
</head>
<body>
  <div class="container">
    <a href="index.html" class="back-link">&larr; Back</a>

    <h1>About</h1>
    <p class="subtitle">Methods, data, and references for the WhaleSAE paper.</p>

    <div style="display:flex;flex-wrap:wrap;gap:0.5em;margin-bottom:1.5em">
      <a href="index.html" style="padding:0.35em 0.8em;border:1px solid #ddd;border-radius:5px;font-size:0.85em;color:#555">Feature explorer</a>
      <a href="codas.html" style="padding:0.35em 0.8em;border:1px solid #ddd;border-radius:5px;font-size:0.85em;color:#555">Coda types</a>
      <a href="docs.html" style="padding:0.35em 0.8em;border:1px solid #ddd;border-radius:5px;font-size:0.85em;color:#555">Guide</a>
      <a href="codalm.html" style="padding:0.35em 0.8em;border:1px solid #ddd;border-radius:5px;font-size:0.85em;color:#555">Grid view</a>
    </div>

    <div class="method-section">
      <h2>Data</h2>
      <p><strong>Pacific dataset.</strong> 23,555 codas from 23 locations across the Pacific Ocean, recorded between 1978 and 2017, assigned to 7 vocal clans (Hersh et al., 2022). Each coda is a 9-dimensional ICI vector (inter-click intervals 1 through 9). We build 32-coda windows by sliding across recording sessions with stride 16, yielding 1,131 to 1,174 windows depending on tokenization.</p>
      <p><strong>Dominica dataset.</strong> 8,719 codas from 2 clans (Gero et al., 2016). Used for cross-ocean transfer evaluation only. The CodaLM and SAE are Pacific-trained; only the logistic regression classifier is fit on Dominica SAE features.</p>
    </div>

    <div class="method-section">
      <h2>Tokenization</h2>
      <p>Both tokenizations use the same transformer architecture and the same SAE. The only difference is the token vocabulary.</p>
      <p><strong>IDcall tokens.</strong> The IDcall pipeline (Hersh et al., 2021) separates codas by click count, fits contaminated mixture models within each group, and assigns probabilistic type labels. This produces ~100 coda types. We use the hard assignments as token IDs.</p>
      <p><strong>K-means tokens.</strong> We apply the same grouping by click count, but cluster with K-means (10-15 clusters per group, ~100 total tokens). Each coda is assigned to its nearest centroid. K-means centroids sit in physical ICI space, so they can label any population's codas by nearest-centroid assignment. This is what enables cross-ocean transfer.</p>
      <p><strong>Continuous ICI vectors.</strong> We also tested raw ICI vectors with MSE loss (no discretization). Temporal transfer dropped to 48.4% and classification to 65.4%. Both discrete tokenizations outperform continuous input.</p>
    </div>

    <div class="method-section">
      <h2>CodaLM</h2>
      <p>A GPT-2-style transformer trained on next-coda prediction with cross-entropy loss. 4 layers, 4 attention heads, d_model = 128, ~800K parameters. Input: sequences of 32 coda tokens from the same recording session. The model learns to predict which coda comes next given the preceding context.</p>
      <div class="diagram">Input window (32 codas) &rarr; CodaLM (4L/4H/d=128) &rarr; next-coda prediction</div>
    </div>

    <div class="method-section">
      <h2>Sparse autoencoder</h2>
      <p>A TopK SAE (Cunningham et al., 2024; Templeton et al., 2024) trained on CodaLM's hidden states. The encoder maps 128-dimensional hidden states to 512 features, with only k=12 active per input (TopK activation). The decoder reconstructs the hidden states from the sparse feature activations.</p>
      <div class="diagram">CodaLM hidden states (128-dim) &rarr; SAE encoder &rarr; 512 features (12 active) &rarr; SAE decoder &rarr; reconstructed states</div>
      <p>Of the 512 features, ~191 are "alive" (fire on more than 1% of windows). The rest are dead features that never activate.</p>
    </div>

    <div class="method-section">
      <h2>Evaluation</h2>
      <p><strong>Classification.</strong> 5-fold StratifiedGroupKFold cross-validation, where all windows from the same recording session stay in the same fold to prevent leakage. SAE classification is averaged over 5 random seeds.</p>
      <p><strong>Temporal transfer.</strong> Split by year (5 splits, 1998-2002). Train CodaLM + SAE on pre-split data, extract SAE features from post-split data, classify with logistic regression. K-means centroids are fit on pre-split data only.</p>
      <p><strong>Cross-ocean transfer.</strong> Push Dominica codas through the Pacific-trained pipeline. For K-means tokens, Dominica codas are assigned to the nearest Pacific centroid. IDcall labels cannot be applied to Dominica.</p>
      <p><strong>Causal steering.</strong> For each clan, identify the most selective SAE feature and inject its decoder direction into CodaLM's residual stream. A steering matrix is degenerate if any single target clan captures over 80% of all steered inputs regardless of source.</p>
    </div>

    <div class="method-section">
      <h2>Selectivity</h2>
      <p>Selectivity measures how concentrated a feature's activation is across clans:</p>
      <div class="diagram">selectivity = mean_activation[dominant_clan] / (mean_activation[dominant_clan] + mean_activation[other_clans])</div>
      <p>A selectivity of 1.0 means the feature fires exclusively for one clan. A selectivity of 0.14 (1/7) means it fires equally across all seven clans.</p>
    </div>

    <div class="method-section">
      <h2>References</h2>
      <ul class="ref-list">
        <li>Hersh, T.A., et al. (2022). <span class="ref-title">Evidence from sperm whale clans of symbolic marking in non-human cultures.</span> PNAS, 119(37).</li>
        <li>Hersh, T.A., et al. (2021). <span class="ref-title">Using identity calls to detect structure in acoustic datasets.</span> Methods in Ecology and Evolution, 12(9).</li>
        <li>Gero, S., Whitehead, H., &amp; Rendell, L. (2016). <span class="ref-title">Individual, unit and vocal clan level identity cues in sperm whale codas.</span> Royal Society Open Science, 3(1).</li>
        <li>Rendell, L. &amp; Whitehead, H. (2003). <span class="ref-title">Vocal clans in sperm whales.</span> Proceedings of the Royal Society B, 270(1512).</li>
        <li>Sharma, P., et al. (2024). <span class="ref-title">Contextual and combinatorial structure in sperm whale vocalisations.</span> Nature Communications, 15.</li>
        <li>Cantor, M., et al. (2015). <span class="ref-title">Multilevel animal societies can emerge from cultural transmission.</span> Nature Communications, 6.</li>
        <li>Cunningham, H., et al. (2024). <span class="ref-title">Sparse Autoencoders Find Highly Interpretable Features in Language Models.</span> ICLR.</li>
        <li>Templeton, A., et al. (2024). <span class="ref-title">Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet.</span> Anthropic.</li>
        <li>Begus, G., Leban, A., &amp; Gero, S. (2023). <span class="ref-title">Approaching an unknown communication system by latent space exploration and causal inference.</span> arXiv:2303.10931.</li>
      </ul>
    </div>

    <hr>

    <p style="font-size:0.88em;color:#666">Code: <a href="https://github.com/garrettallen14/whalesae-paper">github.com/garrettallen14/whalesae-paper</a></p>

    <footer><a href="https://gallen.dev">gallen.dev</a></footer>
  </div>
</body>
</html>
